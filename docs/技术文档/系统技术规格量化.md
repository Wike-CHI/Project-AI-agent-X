# 系统技术规格量化文档

## 1. 系统配置参数

基于以下配置目标：
- **部署环境**：8GB RAM 低配电脑
- **目标用户**：10-100 用户规模
- **响应延迟**：2-5 秒
- **LLM策略**：本地轻量模型 + 云端 MiniMax API

### 1.1 LLM 配置矩阵

| 场景 | 模型 | 量化级别 | 显存需求 | 响应速度 | 适用场景 |
|-----|------|---------|---------|---------|---------|
| **本地主力** | Qwen 2.5 1.5B | Q4 | ~1GB | 快 | 简单对话、情绪识别 |
| **本地增强** | Qwen 2.5 3B | Q4 | ~2GB | 中 | 日常对话、任务处理 |
| **云端主力** | MiniMax API | - | - | 中 | 复杂推理、知识查询 |
| **云端备份** | OpenAI GPT-4o-mini | - | - | 中 | API网关降级 |

### 1.2 嵌入模型配置

| 模型 | 维度 | 参数量 | 内存占用 | 设备 |
|-----|------|-------|---------|------|
| **推荐** all-MiniLM-L6-v2 | 384 | 22M | ~200MB | CPU |
| 备选 bge-small-zh-v1.5 | 512 | 24M | ~300MB | CPU |
| 轻量 GTE-small | 384 | 18M | ~150MB | CPU |

### 1.3 向量数据库配置 (Qdrant)

```yaml
# Qdrant 配置参数
qdrant:
  # 存储模式: 内存 + 磁盘 (低配友好)
  storage:
    storage_type: "memory_map"  # 或 "on_disk"
    on_disk: true

  # 集合配置
  collection:
    name: "pdns_memories"
    vector_size: 384  # 与嵌入模型匹配
    distance: "Cosine"
    hnsw:
      m: 16           # 索引参数
      ef_construct: 100
      full_scan_threshold: 10000

  # 资源限制 (8GB RAM)
  resources:
    max_memory: 2GB
    max_collections: 10
    optimizers:
      deleted_threshold: 0.2
      vacuum_min_vector_number: 1000

  # REST API
  api:
    http_port: 6333
    grpc_port: 6334
```

### 1.4 图数据库配置 (Neo4j)

```yaml
# Neo4j 配置参数
neo4j:
  # 连接配置
  uri: "bolt://localhost:7687"
  database: "neo4j"

  # 资源限制 (低配优化)
  resources:
    memory:
      pagecache: 512MB
      heap: 1GB

  # 社区版限制
  community: true
```

### 1.5 关系数据库配置 (SQLite)

```python
# SQLite 配置
SQLITE_CONFIG = {
    "path": "data/memory/relational.db",
    "timeout": 30.0,
    "check_same_thread": False,
    "journal_mode": "WAL",  # 写性能优化
    "cache_size": -64000,   # 64MB 缓存
}
```

## 2. 性能指标

### 2.1 延迟分解 (目标 < 3秒)

| 阶段 | 本地模型 | 云端模型 | 优化策略 |
|-----|---------|---------|---------|
| 输入处理 | 50ms | 50ms | 异步预处理 |
| 记忆检索 | 200ms | 200ms | Qdrant缓存 |
| 嵌入生成 | 300ms | 300ms | 批量处理 |
| **LLM推理** | **800ms** | **1000ms** | 量化模型 |
| 后处理 | 100ms | 100ms | 流式输出 |
| **总计** | **~1.5秒** | **~2秒** |  |

### 2.2 吞吐量指标

| 指标 | 值 | 说明 |
|-----|-----|-----|
| 并发用户 | 20-50 | 8GB RAM 配置 |
| 每秒请求数 (RPS) | 10-20 | 混合本地/云端 |
| 记忆检索延迟 P99 | < 500ms | Qdrant 内存模式 |
| 对话响应延迟 P95 | < 3秒 | 含完整 RAG 流程 |

### 2.3 存储容量规划

| 数据类型 | 单用户预估 | 100用户总计 | 存储建议 |
|---------|-----------|------------|---------|
| 对话历史 | 10MB/月 | 1GB | SQLite 压缩 |
| 语义记忆 | 50MB/年 | 5GB | Qdrant 磁盘存储 |
| 情景记忆 | 5MB/年 | 500MB | Neo4j 社区版 |
| 向量索引 | 100MB/年 | 10GB | Qdrant 优化 |
| **总计** | | **~16.5GB** | |

## 3. 资源预算 (8GB RAM)

```
┌────────────────────────────────────────────────────────────┐
│                    8GB RAM 分配方案                          │
├────────────────────────────────────────────────────────────┤
│                                                            │
│  操作系统            500MB ████████░░░░░░░░░░░░░░░░░░░░░░  │
│  Qdrant 向量存储    1000MB ████████████████░░░░░░░░░░░░░░  │
│  Neo4j 图数据库     1500MB ████████████████████████████░░  │
│  SQLite + 缓存       500MB ████████░░░░░░░░░░░░░░░░░░░░░░  │
│  本地 LLM (Qwen 3B) 2500MB █████████████████████████████░  │
│  嵌入模型            300MB █████░░░░░░░░░░░░░░░░░░░░░░░░░░  │
│  Python/应用        1200MB ██████████████████████░░░░░░░░  │
│  ----------------------------------------------            │
│  预留余量            500MB ████████░░░░░░░░░░░░░░░░░░░░░░  │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

## 4. API 网关设计

### 4.1 网关架构

```
┌─────────────────────────────────────────────────────────────┐
│                      API Gateway                              │
│  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐          │
│  │ 路由分发 │ │ 负载均衡 │ │ 限流熔断 │ │ 认证授权 │          │
│  └─────────┘ └─────────┘ └─────────┘ └─────────┘          │
└─────────────────────────────────────────────────────────────┘
                              │
        ┌─────────────────────┼─────────────────────┐
        ▼                     ▼                     ▼
┌───────────────┐   ┌───────────────┐   ┌───────────────┐
│  本地 Ollama   │   │ MiniMax API   │   │  OpenAI API   │
│  Qwen 1.5B Q4  │   │ (云端主力)    │   │  (备用)       │
└───────────────┘   └───────────────┘   └───────────────┘
```

### 4.2 模型切换配置

```python
# backend/src/pdns/llm/gateway.py

from enum import Enum
from dataclasses import dataclass
from typing import Optional

class LLMProvider(Enum):
    LOCAL = "local"
    MINIMAX = "minimax"
    OPENAI = "openai"

@dataclass
class ModelConfig:
    name: str
    provider: LLMProvider
    max_tokens: int
    temperature: float
    timeout: int
    cost_per_token: float  # 云端API计费

LLM_CONFIGS = {
    # 本地模型 (免费)
    "qwen2.5:1.5b": ModelConfig(
        name="Qwen 2.5 1.5B",
        provider=LLMProvider.LOCAL,
        max_tokens=2048,
        temperature=0.7,
        timeout=30,
        cost_per_token=0.0
    ),
    # 云端 MiniMax
    "minimax-abab6.5s": ModelConfig(
        name="MiniMax ABAB 6.5s",
        provider=LLMProvider.MINIMAX,
        max_tokens=16384,
        temperature=0.7,
        timeout=60,
        cost_per_token=0.00001  # 示例价格
    ),
    # 备用 OpenAI
    "gpt-4o-mini": ModelConfig(
        name="GPT-4o-mini",
        provider=LLMProvider.OPENAI,
        max_tokens=16384,
        temperature=0.7,
        timeout=60,
        cost_per_token=0.00015
    )
}
```

### 4.3 自动故障转移

```python
class LLMGateway:
    """LLM 网关 - 自动故障转移"""

    def __init__(self):
        self.providers = self._init_providers()
        self.fallback_order = [
            "qwen2.5:1.5b",  # 本地优先
            "minimax-abab6.5s",
            "gpt-4o-mini"
        ]

    async def generate(
        self,
        prompt: str,
        preferred_model: str = None,
        fallback: bool = True
    ) -> str:
        """智能生成 - 支持故障转移"""

        # 尝试首选模型
        model = preferred_model or "qwen2.5:1.5b"

        for attempt_model in [model] + (self.fallback_order if fallback else []):
            provider = self.providers[attempt_model]

            try:
                if provider.is_available():
                    return await provider.generate(prompt)
            except Exception as e:
                log.warning(f"Model {attempt_model} failed: {e}")
                continue

        raise RuntimeError("All models failed")
```

## 5. 部署配置

### 5.1 Docker Compose (低配优化)

```yaml
# docker-compose.yml

services:
  # 向量数据库 - 内存优化
  qdrant:
    image: qdrant/qdrant:v1.12.0
    container_name: pdns-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./data/qdrant:/qdrant/storage
    environment:
      - QDRANT__STORAGE__ON_DISK=true
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    mem_limit: 1.5g
    deploy:
      resources:
        limits:
          memory: 1.5g
    restart: unless-stopped

  # 图数据库 - 社区版
  neo4j:
    image: neo4j:5.20.0-community
    container_name: pdns-neo4j
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - ./data/neo4j/data:/data
      - ./data/neo4j/logs:/logs
    environment:
      - NEO4J_AUTH=neo4j/password
      - NEO4J_dbms_memory_heap_initial__size=512m
      - NEO4J_dbms_memory_heap_max__size=1g
      - NEO4J_dbms_pagecache_size=512m
    mem_limit: 1.5g
    restart: unless-stopped

  # 后端服务
  backend:
    build: ./backend
    container_name: pdns-backend
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app
      - ./data:/app/data
    environment:
      - ENV=production
      - LOG_LEVEL=INFO
    depends_on:
      - qdrant
      - neo4j
    mem_limit: 2g
    restart: unless-stopped

  # 前端 (可选)
  frontend:
    build: ./frontend
    container_name: pdns-frontend
    ports:
      - "3000:3000"
    mem_limit: 512m
    restart: unless-stopped
```

### 5.2 环境变量模板

```bash
# .env.example

# 应用配置
APP_NAME=Personal Digital Nervous System
APP_VERSION=0.1.0
DEBUG=false
LOG_LEVEL=INFO

# LLM 配置
LLM_PROVIDER=local  # local, minimax, openai
LLM_MODEL=qwen2.5:1.5b
OLLAMA_BASE_URL=http://ollama:11434

# MiniMax API (云端)
MINIMAX_API_URL=https://api.minimax.chat/v1
MINIMAX_API_KEY=your_api_key_here

# OpenAI API (备用)
OPENAI_API_KEY=your_api_key_here

# Qdrant 配置
QDRANT_HOST=qdrant
QDRANT_PORT=6333
QDRANT_GRPC_PORT=6334
QDRANT_COLLECTION_NAME=pdns_memories

# Neo4j 配置
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password
NEO4J_DATABASE=neo4j

# 嵌入模型
EMBEDDING_MODEL=all-MiniLM-L6-v2
EMBEDDING_DIMENSIONS=384
EMBEDDING_DEVICE=cpu

# API 服务
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=1

# 资源限制
MAX_MEMORY_USAGE=6GB
MAX_CONCURRENT_USERS=20
```

## 6. 性能监控指标

### 6.1 关键指标

| 指标 | 告警阈值 | 目标值 |
|-----|---------|-------|
| 响应延迟 P95 | > 5秒 | < 3秒 |
| 内存使用率 | > 85% | < 70% |
| Qdrant 查询延迟 | > 200ms | < 100ms |
| Neo4j 查询延迟 | > 500ms | < 200ms |
| API 错误率 | > 1% | < 0.1% |

### 6.2 监控端点

```python
# /api/v1/metrics
{
    "system": {
        "memory_usage_mb": 4096,
        "cpu_percent": 45.2,
        "disk_usage_percent": 32.1
    },
    "llm": {
        "local_requests": 1250,
        "cloud_requests": 89,
        "avg_latency_ms": 1450,
        "fallback_count": 3
    },
    "database": {
        "qdrant_vectors": 15420,
        "neo4j_nodes": 3250,
        "sqlite_records": 8920
    }
}
```

## 7. 文件结构更新

```
backend/src/pdns/
├── __init__.py
├── config/
│   ├── __init__.py
│   ├── settings.py              # 主配置
│   └── llm_config.py            # LLM配置 + API网关
├── agents/                      # 智能体模块
├── crews/                       # 团队模块
├── memory/                      # 记忆模块
├── rag/                         # RAG模块
├── api/                         # API模块
├── llm/                         # 新增: LLM网关模块
│   ├── __init__.py
│   ├── gateway.py               # API网关
│   ├── providers/               # 模型提供者
│   │   ├── __init__.py
│   │   ├── local.py             # Ollama本地
│   │   ├── minimax.py           # MiniMax云端
│   │   └── openai.py            # OpenAI备用
│   └── router.py                # 智能路由
└── utils/                       # 工具模块
```
